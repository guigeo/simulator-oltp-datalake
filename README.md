# ğŸ¥ Alimentador-BD

**OLTP Hospital Simulator** â€” Continuous data streaming for CDC testing with Debezium or another CDC ingestion engine.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Python 3.11+](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/)
[![PostgreSQL 14+](https://img.shields.io/badge/PostgreSQL-14+-336791.svg)](https://www.postgresql.org/)

---

## ğŸ¯ Overview

Alimentador-BD is a production-ready Python simulator that generates **realistic hospital data** in PostgreSQL with continuous INSERT/UPDATE operations. Perfect for testing **CDC (Change Data Capture)** pipelines with Debezium, validating data consistency, and developing ETL/ELT systems.

### Key Features

âœ¨ **Continuous Data Streaming**
- 70% INSERT operations (new records)
- 30% UPDATE operations (realistic modifications)
- ~1 operation per 2 seconds (configurable)

ğŸ¥ **Realistic Hospital Schema**
- 7 OLTP tables (patients, doctors, appointments, exams, admissions, etc.)
- ~13k initial seed records
- Proper foreign keys and constraints
- CDC-compatible triggers and indexes

ğŸ **Production-Ready Code**
- Type hints, docstrings, PEP 8 compliance
- Error handling with exponential backoff
- Batch operations with transaction support
- Comprehensive logging with rotation

ğŸ³ **Multiple Deployment Options**
- Local development (Docker Compose)
- AWS EC2 / RDS
- Kubernetes
- Standalone Python

ğŸ“š **Comprehensive Documentation**
- Quick start (5 minutes)
- Complete user guide (Portuguese)
- Technical architecture
- Production deployment guide
- Developer contribution guide

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11+
- PostgreSQL 14+
- Docker & Docker Compose (optional)

### 1. Setup (5 minutes)

```bash
# Clone repository
git clone https://github.com:Hycky/oltp-simulator.git
cd alimentador-bd

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure (copy example and edit credentials)
cp config/.env.example config/.env
# Edit config/.env with your PostgreSQL connection
```

### 2. Initialize Database

```bash
make init    # Create schema and indexes
make seed    # Populate ~13k initial records
```

### 3. Start Streaming

```bash
make stream  # Continuous INSERT/UPDATE operations
```

### 4. Monitor

```bash
# In another terminal
make counts  # Show record counts
tail -f logs/app.log  # View live logs
```

### With Docker Compose

```bash
# Start PostgreSQL + PgAdmin
docker-compose up -d postgres

# From host machine, initialize
make init
make seed

# Stream
make stream
```

---

## ğŸ“‹ Available Commands

| Command | Description |
|---------|-------------|
| `make install` | Create venv and install dependencies |
| `make init` | Create schema, indexes, lookup data |
| `make seed` | Populate ~13k initial records |
| `make stream` | Start continuous streaming |
| `make reset` | Drop + recreate + seed all |
| `make counts` | Display table record counts |
| `make fmt` | Format code with Black |
| `make lint` | Check code with Ruff |
| `make clean` | Remove cache and temp files |

---

## ğŸ›ï¸ Database Schema

### 7 OLTP Tables

```sql
pacientes (2,000)
â”œâ”€â”€ id, nome, nascimento, cpf, telefone, endereco
â”œâ”€â”€ created_at, updated_at (automatic)
â””â”€â”€ PRIMARY KEY, UNIQUE(cpf), INDEX(cpf)

medicos (200)
â”œâ”€â”€ id, nome, crm, especialidade, telefone
â””â”€â”€ PRIMARY KEY, UNIQUE(crm), INDEX(crm)

convenios (12)
â”œâ”€â”€ id, nome, cnpj, tipo, cobertura
â””â”€â”€ PRIMARY KEY, UNIQUE(cnpj)

pacientes_convenios (2,500+)
â”œâ”€â”€ id, paciente_id â†’ pacientes
â”œâ”€â”€ convenio_id â†’ convenios
â””â”€â”€ UNIQUE(paciente_id, convenio_id)

consultas (4,000+)
â”œâ”€â”€ id, paciente_id â†’ pacientes
â”œâ”€â”€ medico_id â†’ medicos
â”œâ”€â”€ data, motivo, status (agendada|realizada|cancelada|faltou)
â””â”€â”€ INDEX(paciente_id, medico_id, data)

exames (3,500+)
â”œâ”€â”€ id, paciente_id â†’ pacientes
â”œâ”€â”€ tipo_exame, data, resultado
â””â”€â”€ INDEX(paciente_id, data)

internacoes (1,200+)
â”œâ”€â”€ id, paciente_id â†’ pacientes
â”œâ”€â”€ data_entrada, data_saida, motivo, quarto
â””â”€â”€ CHECK(data_saida >= data_entrada)
```

### Key Features

- âœ… **BIGSERIAL primary keys** on all tables
- âœ… **Unique constraints** on natural keys (CPF, CRM, CNPJ)
- âœ… **Cascading foreign keys** (ON UPDATE CASCADE, ON DELETE RESTRICT)
- âœ… **Automatic timestamps** with triggers (`created_at`, `updated_at`)
- âœ… **9 strategic indexes** for performance
- âœ… **CDC-compatible** schema for Debezium

---

## âš™ï¸ Configuration

### Environment Variables (`.env`)

```env
# PostgreSQL Connection
PG_HOST=localhost
PG_PORT=5432
PG_USER=postgres
PG_PASSWORD=postgres
PG_DATABASE=teste_pacientes

# Streaming Configuration
STREAM_INTERVAL_SECONDS=2      # Delay between operations (seconds)
BATCH_SIZE=50                  # Records per batch
MAX_JITTER_MS=400              # Random delay variation (ms)

# Seeding Configuration
SEED_PACIENTES=2000
SEED_MEDICOS=200
SEED_CONVENIOS=12
SEED_CONSULTAS=4000
SEED_EXAMES=3500
SEED_INTERNACOES=1200
SEED_PACIENTES_CONVENIOS=2500

# Logging
LOG_LEVEL=INFO                 # DEBUG, INFO, WARNING, ERROR
```

### TOML Configuration (`config/settings.toml`)

```toml
[db]
search_path = "public"
connect_timeout = 10

[stream]
interval_seconds = 2
batch_size = 50
max_jitter_ms = 400
fail_fast_on_critical = true

[logging]
level = "INFO"
rotate_when = "midnight"
backup_count = 7
```

---

## ğŸ”„ Streaming Operations

The simulator executes **8 realistic operations**:

### INSERTs (70%)
1. **insert_paciente** - Register new patient
2. **insert_consulta** - Schedule new appointment
3. **insert_exame** - Request new lab test
4. **insert_internacao** - Admit patient to hospital

### UPDATEs (30%)
5. **update_paciente** - Modify contact info
6. **update_consulta** - Change appointment status
7. **update_exame** - Record lab results
8. **update_internacao** - Discharge patient

Each operation:
- âœ… Validates foreign keys before execution
- âœ… Commits in batches for performance
- âœ… Logs operation type and counts
- âœ… Handles errors gracefully (continues on non-critical failures)
- âœ… Reconnects automatically with exponential backoff

---

## ğŸ§ª Testing & Validation

### Verify Data Consistency

```sql
-- Check for orphaned records (should return 0)
SELECT COUNT(*) FROM consultas 
WHERE paciente_id NOT IN (SELECT id FROM pacientes);

-- Verify unique CPFs
SELECT cpf, COUNT(*) FROM pacientes 
GROUP BY cpf HAVING COUNT(*) > 1;

-- Check timestamp coherence
SELECT COUNT(*) FROM consultas 
WHERE created_at > now();
```

### Monitor Growth

```bash
# Terminal 1 - Stream for 5 minutes
timeout 300 make stream

# Terminal 2 - Check growth every 10 seconds
while true; do make counts; sleep 10; done
```

### Performance Testing

```bash
# Stress test: high throughput
STREAM_INTERVAL_SECONDS=0 timeout 60 make stream

# Measure: ~200 ops/minute
```

---

## ğŸ”Œ Debezium / CDC Integration

Alimentador-BD generates **CDC-compatible changes** for Debezium capture.

### Debezium Configuration

```json
{
  "name": "postgres-connector",
  "config": {
    "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
    "database.hostname": "10.42.88.67",
    "database.port": 5441,
    "database.user": "app",
    "database.password": "app123",
    "database.dbname": "teste_pacientes",
    "database.server.name": "alimentador-bd",
    "plugin.name": "pgoutput",
    "publication.name": "alimentador_pub",
    "table.include.list": "public.*",
    "publication.autocreate.mode": "filtered",
    "slot.name": "alimentador_slot"
  }
}
```

### What Gets Captured

- âœ… All INSERT operations â†’ `{before: null, after: {patient data}}`
- âœ… All UPDATE operations â†’ `{before: {old data}, after: {new data}}`
- âœ… `updated_at` field automatically populated by triggers
- âœ… Natural keys (CPF, CRM, CNPJ) for deduplication

### Expected Kafka Events

```json
{
  "schema": {...},
  "payload": {
    "before": null,
    "after": {
      "id": 2045,
      "nome": "JoÃ£o Silva",
      "cpf": "123.456.789-00",
      "created_at": 1705255200000,
      "updated_at": 1705255200000
    },
    "source": {
      "version": "2.4.0.Final",
      "connector": "postgresql",
      "name": "alimentador-bd",
      "ts_ms": 1705255200123,
      "txId": 12345,
      "lsn": 12345678,
      "xmin": null
    },
    "op": "c",
    "ts_ms": 1705255200123,
    "transaction": null
  }
}
```

---

## ğŸ“ Project Structure

```
alimentador_bd/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ .env.example          # Template for credentials
â”‚   â””â”€â”€ settings.toml         # Configuration
â”œâ”€â”€ scripts/                  # Python modules
â”‚   â”œâ”€â”€ cli.py               # CLI interface (Typer)
â”‚   â”œâ”€â”€ stream.py            # Streaming engine
â”‚   â”œâ”€â”€ seed.py              # Initial data population
â”‚   â”œâ”€â”€ db_init.py           # Database connection
â”‚   â”œâ”€â”€ data_gen.py          # Data generation (Faker)
â”‚   â”œâ”€â”€ validators.py        # FK validation cache
â”‚   â””â”€â”€ reset.py             # Reset orchestration
â”œâ”€â”€ sql/                      # SQL scripts
â”‚   â”œâ”€â”€ 01_schema.sql        # Table definitions
â”‚   â”œâ”€â”€ 02_indexes.sql       # Indexes
â”‚   â”œâ”€â”€ 03_seed-lookups.sql  # Initial data
â”‚   â””â”€â”€ 99_drop_all.sql      # Cleanup
â”œâ”€â”€ logs/                     # Runtime logs
â”œâ”€â”€ Makefile                  # Build automation
â”œâ”€â”€ Dockerfile                # Container image
â”œâ”€â”€ docker-compose.yml        # Local stack
â”œâ”€â”€ pyproject.toml           # Python config
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ README.md                # This file
â”œâ”€â”€ GUIDE.md                 # User guide (Portuguese)
â”œâ”€â”€ ARCHITECTURE.md          # Technical design
â”œâ”€â”€ DEPLOYMENT.md            # Production setup
â”œâ”€â”€ CONTRIBUTING.md          # Contribution guide
â”œâ”€â”€ CHANGELOG.md             # Version history
â””â”€â”€ LICENSE                  # MIT license
```

---

## ğŸ³ Docker Deployment

### Run Locally

```bash
# Start PostgreSQL (Docker)
docker-compose up -d postgres

# Initialize from host
make init
make seed

# Stream
make stream
```

### Build Image

```bash
docker build -t alimentador-bd:1.0.0 .

docker run --rm \
  -e PG_HOST=localhost \
  -e PG_USER=app \
  -e PG_PASSWORD=app123 \
  -e PG_DATABASE=teste_pacientes \
  -v ./logs:/app/logs \
  alimentador-bd:1.0.0 \
  python -m scripts.cli stream
```

---

## â˜ï¸ Production Deployment

See **[DEPLOYMENT.md](DEPLOYMENT.md)** for detailed guides:
- âœ… AWS EC2 + RDS setup
- âœ… Kubernetes deployment
- âœ… Monitoring and scaling
- âœ… Backup and recovery
- âœ… Security best practices

---

## ğŸ“š Documentation

| Document | Purpose |
|----------|---------|
| [**README.md**](README.md) | Overview, quick start, schema (this file) |
| [**GUIDE.md**](GUIDE.md) | Complete user manual in Portuguese ğŸ‡§ğŸ‡· |
| [**ARCHITECTURE.md**](ARCHITECTURE.md) | Technical design and data flow |
| [**DEPLOYMENT.md**](DEPLOYMENT.md) | Production setup (AWS, K8s, Docker) |
| [**CONTRIBUTING.md**](CONTRIBUTING.md) | How to contribute, dev setup |
| [**CHANGELOG.md**](CHANGELOG.md) | Version history and roadmap |

---

## ğŸ› Troubleshooting

### Connection Error: "connection refused"

```bash
# Check PostgreSQL is running
psql -U postgres -h localhost -c "SELECT 1"

# Verify credentials in config/.env
cat config/.env | grep PG_
```

### IntegrityError: "duplicate key value"

This is **expected and handled gracefully**. The simulator skips duplicates and logs them:

```bash
grep "IntegrityError" logs/app.log
```

### Stream not starting

```bash
# Verify database is initialized
make init
make seed
make counts

# Check logs
tail -20 logs/app.log
```

### Slow inserts

```bash
# Check disk space and PostgreSQL performance
df -h
psql -U app -d teste_pacientes -c "SELECT * FROM pg_stat_user_tables"

# Reduce batch size if needed
BATCH_SIZE=25 make stream
```

---

## ğŸ¤ Contributing

We welcome contributions! See [**CONTRIBUTING.md**](CONTRIBUTING.md) for:
- Development setup
- Code style guidelines
- Testing procedures
- Pull request workflow

Quick start for contributors:

```bash
git clone https://github.com/yourusername/alimentador-bd.git
cd alimentador-bd
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
make init && make seed
make stream  # Test it works
```

---

## ğŸ“Š Performance Metrics

| Metric | Value |
|--------|-------|
| Seed time | <2 seconds |
| Initial records | ~13,000 |
| Stream rate | 1 op / 2s |
| Batch size | 50 records |
| Insert ops | 70% |
| Update ops | 30% |
| Throughput | 200+ ops/min |
| Memory usage | ~256 MB |
| CPU usage | Low (<1 core) |

---

## ğŸ“ License

MIT License - See [LICENSE](LICENSE) for details

**Copyright Â© 2025 Henrique Ferreira**

---

## ğŸ“ Support

- **Documentation**: See [GUIDE.md](GUIDE.md) (Portuguese) or [ARCHITECTURE.md](ARCHITECTURE.md) (English)
- **Issues**: Report bugs using GitHub issue templates
- **Discussions**: Ask questions in GitHub Discussions
- **Email**: [your-email@example.com]

---

## ğŸ‰ Next Steps

1. **Read** [GUIDE.md](GUIDE.md) (Portuguese user guide) or this README
2. **Setup** with `make install && make init && make seed`
3. **Run** with `make stream`
4. **Monitor** with `make counts` and `tail -f logs/app.log`
5. **Deploy** using [DEPLOYMENT.md](DEPLOYMENT.md) for production

---

**Version**: 1.0.0 | **Status**: Production Ready âœ… | **License**: MIT
